{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets -q"
      ],
      "metadata": {
        "id": "ca9vPWZ6cjBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ALmkUTRI2q5J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from dataclasses import dataclass\n",
        "import copy\n",
        "import wandb\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from datasets import load_dataset\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('Intel/bert-base-uncased-mrpc')\n",
        "bert_model = AutoModelForSequenceClassification.from_pretrained(\"Intel/bert-base-uncased-mrpc\").to(device)\n",
        "bert_model.eval()"
      ],
      "metadata": {
        "id": "WQXjmFW37J3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bert_model)\n",
        "print(sum(p.numel() for p in bert_model.parameters())) # 110M"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "id": "H-CnpTF0844i",
        "outputId": "74d37ef0-989e-4255-c2a7-6c4cce69aef8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BertForSequenceClassification(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSdpaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
            ")\n",
            "109483778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(bert_model.bert.embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QregcoPbC0Kv",
        "outputId": "4dff3c1d-e28a-4e37-c925-0ac41ae3b4da"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BertEmbeddings(\n",
            "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "  (position_embeddings): Embedding(512, 768)\n",
            "  (token_type_embeddings): Embedding(2, 768)\n",
            "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class DistBERTConfig:\n",
        "    bert_hidden_size: int = 768\n",
        "    num_blocks: int = 4\n",
        "    num_heads: int = 12\n",
        "    intermediate_size: int = 1024\n",
        "    dropout: float = 0.1\n",
        "\n",
        "config = DistBERTConfig()"
      ],
      "metadata": {
        "id": "_BPRpdzG7wwL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Часть 1. Дистиляция в fp16"
      ],
      "metadata": {
        "id": "hufebD3lgcsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.mha = nn.MultiheadAttention(\n",
        "            embed_dim=config.bert_hidden_size,\n",
        "            num_heads=config.num_heads,\n",
        "            dropout=config.dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(config.bert_hidden_size, config.intermediate_size),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(config.intermediate_size, config.bert_hidden_size)\n",
        "        )\n",
        "\n",
        "        self.layernorm1 = nn.LayerNorm(config.bert_hidden_size, eps=1e-12)\n",
        "        self.layernorm2 = nn.LayerNorm(config.bert_hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x, attention_mask=None, return_attention=False):\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = ~attention_mask.bool()\n",
        "        else:\n",
        "            attention_mask = None\n",
        "\n",
        "        mha_out, attn_weights = self.mha(\n",
        "            query=x,\n",
        "            key=x,\n",
        "            value=x,\n",
        "            key_padding_mask=attention_mask,\n",
        "            need_weights=return_attention,\n",
        "            average_attn_weights=False\n",
        "        )\n",
        "\n",
        "        residual = x + self.dropout(mha_out)\n",
        "        x = self.layernorm1(residual)\n",
        "\n",
        "        ffn_out = self.ffn(x)\n",
        "        residual = x + self.dropout(ffn_out)\n",
        "        output = self.layernorm2(residual)\n",
        "\n",
        "        if return_attention:\n",
        "            return output, attn_weights\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "class CleanBERT(nn.Module):\n",
        "    def __init__(self, embedding_layer, config, num_labels=2):\n",
        "        super().__init__()\n",
        "        self.embeddings = embedding_layer\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(config) for _ in range(config.num_blocks)\n",
        "        ])\n",
        "\n",
        "        self.pooler = nn.Sequential(\n",
        "            nn.Linear(config.bert_hidden_size, config.bert_hidden_size),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.classifier = nn.Linear(config.bert_hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None,\n",
        "                return_hidden_states=False, return_attentions=False):\n",
        "        x = self.embeddings(input_ids)\n",
        "\n",
        "        all_hidden_states = []\n",
        "        all_attentions = []\n",
        "\n",
        "        if return_hidden_states:\n",
        "            all_hidden_states.append(x)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            if return_attentions:\n",
        "                x, attn = block(x, attention_mask=attention_mask, return_attention=True)\n",
        "                all_attentions.append(attn)\n",
        "            else:\n",
        "                x = block(x, attention_mask=attention_mask, return_attention=False)\n",
        "\n",
        "            if return_hidden_states:\n",
        "                all_hidden_states.append(x)\n",
        "\n",
        "        cls_output = x[:, 0]\n",
        "\n",
        "        pooled = self.pooler(cls_output)\n",
        "        pooled = self.dropout(pooled)\n",
        "        logits = self.classifier(pooled)\n",
        "\n",
        "        outputs = {\"logits\": logits}\n",
        "\n",
        "        if return_hidden_states:\n",
        "            outputs[\"hidden_states\"] = all_hidden_states\n",
        "\n",
        "        if return_attentions:\n",
        "            outputs[\"attentions\"] = all_attentions\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "RB2qF0e-7j_D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distil_model = CleanBERT(copy.deepcopy(bert_model.bert.embeddings), config).to(device)\n",
        "for param in distil_model.embeddings.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "71W6qXtYGx0j"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_pairs = [\n",
        "    [\"Hello, how are you?\", \"Hi, what's up?\"],\n",
        "    [\"Transformers are amazing\", \"BERT is a powerful model\"],\n",
        "    [\"Distill BERT into a smaller model\", \"Make BERT lighter while preserving performance\"],\n",
        "    [\"We reduce dimensions but keep performance\", \"Performance stays the same after compression\"],\n",
        "]\n",
        "\n",
        "inputs = tokenizer(\n",
        "    text=[pair[0] for pair in sentence_pairs],\n",
        "    text_pair=[pair[1] for pair in sentence_pairs],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"pt\"\n",
        ").to(device)\n",
        "\n",
        "out = distil_model(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"],\n",
        "                   return_hidden_states=True, return_attentions=True)\n",
        "logits, hidden_states, attentions = out[\"logits\"], out[\"hidden_states\"], out[\"attentions\"]\n",
        "print(logits, len(hidden_states), hidden_states[0].shape, len(attentions), attentions[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12iOpIIhIJVa",
        "outputId": "cd5ceb42-40da-43a6-fabc-9d8a6cac8c53"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0445, 0.4749],\n",
            "        [0.0869, 0.2349],\n",
            "        [0.2543, 0.0531],\n",
            "        [0.0382, 0.2491]], device='cuda:0', grad_fn=<AddmmBackward0>) 5 torch.Size([4, 17, 768]) 4 torch.Size([4, 12, 17, 17])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(distil_model)\n",
        "print(sum(p.numel() for p in distil_model.parameters())) # 40M (0.36 * BertOrigNumParam)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilN8FHv-Hf9b",
        "outputId": "17c695ff-c17b-47b7-8379-eeb151dac2b5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CleanBERT(\n",
            "  (embeddings): BertEmbeddings(\n",
            "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "    (position_embeddings): Embedding(512, 768)\n",
            "    (token_type_embeddings): Embedding(2, 768)\n",
            "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (blocks): ModuleList(\n",
            "    (0-3): 4 x TransformerBlock(\n",
            "      (mha): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "      )\n",
            "      (ffn): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=1024, bias=True)\n",
            "        (1): GELU(approximate='none')\n",
            "        (2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "      )\n",
            "      (layernorm1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (layernorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (pooler): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (1): Tanh()\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
            ")\n",
            "40189698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss = a * task_loss (CrossEntropy) + b * logits_loss (KLDiv) + c * hidden_states (MSE) + d * attention_loss (KLDiv)\n",
        "\n",
        "Чтобы посчитать hidden_states и attention_loss нужен маппинг слоёв, о нём ниже."
      ],
      "metadata": {
        "id": "gNp8_NcHNpfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    outputs = bert_model(\n",
        "        inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        output_hidden_states=True,\n",
        "        output_attentions=True\n",
        "    )\n",
        "\n",
        "# Получаем hidden_states: tuple из 13 элементов (embeddings + 12 слоёв)\n",
        "hidden_states = outputs.hidden_states\n",
        "attentions = outputs.attentions\n",
        "hidden_states[12].shape\n",
        "print(len(hidden_states), hidden_states[0].shape)\n",
        "print(len(attentions), attentions[0].shape)"
      ],
      "metadata": {
        "id": "tD5a5G0WKbAo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b8e0a85-1371-47f7-e85e-3231193dd284"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13 torch.Size([4, 17, 768])\n",
            "12 torch.Size([4, 12, 17, 17])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(batch_size=16):\n",
        "    dataset = load_dataset(\"nyu-mll/glue\", \"mrpc\")\n",
        "\n",
        "    def tokenize(batch):\n",
        "        return tokenizer(\n",
        "            batch[\"sentence1\"],\n",
        "            batch[\"sentence2\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "    encoded_dataset = dataset.map(tokenize, batched=True, batch_size=batch_size)\n",
        "    encoded_dataset = encoded_dataset.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
        "    encoded_dataset = encoded_dataset.rename_column(\"label\", \"labels\")\n",
        "    encoded_dataset.set_format(\"torch\")\n",
        "\n",
        "    train_loader = DataLoader(encoded_dataset[\"train\"], batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(encoded_dataset[\"test\"], batch_size=batch_size)\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "def kl_div_loss(student_logits, teacher_logits, temperature=2.3):\n",
        "    student_log_probs = F.log_softmax(student_logits / temperature, dim=1)\n",
        "    teacher_probs = F.softmax(teacher_logits / temperature, dim=1)\n",
        "    return F.kl_div(student_log_probs, teacher_probs, reduction='batchmean') * (temperature ** 2)\n",
        "\n",
        "def hidden_state_loss(student_hs, teacher_hs):\n",
        "    return F.mse_loss(student_hs, teacher_hs.detach())\n",
        "\n",
        "def attention_kl_loss(student_attns, teacher_attns, mapping_attn, temperature=2.3):\n",
        "    total_loss = 0.0\n",
        "    num_layers = len(student_attns)\n",
        "\n",
        "    for student_idx in range(num_layers):\n",
        "        student_attn = student_attns[student_idx]  # [B, H, T, T]\n",
        "        teacher_attn = teacher_attns[mapping_attn[student_idx]]  # [B, H, T, T]\n",
        "\n",
        "        B, H, T, _ = student_attn.shape\n",
        "\n",
        "        # Применяем температурное масштабирование\n",
        "        student_attn = student_attn / temperature\n",
        "        teacher_attn = teacher_attn / temperature\n",
        "\n",
        "        # Вычисляем логарифмы вероятностей и вероятности\n",
        "        student_log_probs = F.log_softmax(student_attn, dim=-1)  # [B, H, T, T]\n",
        "        teacher_probs = F.softmax(teacher_attn, dim=-1)  # [B, H, T, T]\n",
        "\n",
        "        # Вычисляем KL-дивергенцию для каждого элемента\n",
        "        kl_per_element = F.kl_div(\n",
        "            student_log_probs,\n",
        "            teacher_probs,\n",
        "            reduction='none'\n",
        "        )  # [B, H, T, T]\n",
        "\n",
        "        # Суммируем по последнему измерению (T)\n",
        "        kl_per_token = kl_per_element.sum(dim=-1)  # [B, H, T]\n",
        "\n",
        "        # Суммируем по всем головам и токенам (но не по батчу!)\n",
        "        kl_per_layer = kl_per_token.sum(dim=(1, 2))  # [B]\n",
        "\n",
        "        # Усредняем по батчу\n",
        "        layer_loss = kl_per_layer.mean()\n",
        "\n",
        "        # Масштабируем обратно температурой\n",
        "        layer_loss = layer_loss * (temperature ** 2)\n",
        "\n",
        "        total_loss += layer_loss\n",
        "\n",
        "    return total_loss / num_layers"
      ],
      "metadata": {
        "id": "V51YfkU0ULrY"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"bert_mrpc_fp16dist\", config={\n",
        "    \"a\": 4,\n",
        "    \"b\": 1,\n",
        "    \"c\": 0.3,\n",
        "    \"d\": 0.5,\n",
        "    \"batch_size\": 8,\n",
        "    \"epochs\": 13,\n",
        "    \"lr\": 6e-5,\n",
        "    \"temperature\": 1.25\n",
        "})"
      ],
      "metadata": {
        "id": "6rmCES1osH59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_distil_model(student_model, teacher_model, tokenizer, device):\n",
        "    train_loader, val_loader = prepare_data(batch_size=wandb.config.batch_size)\n",
        "\n",
        "    optimizer = AdamW(student_model.parameters(), lr=wandb.config.lr)\n",
        "    total_steps = len(train_loader) * wandb.config.epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=100, num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    step = 0\n",
        "\n",
        "    scaler = GradScaler('cuda')\n",
        "\n",
        "    mapping_attn = [2, 5, 8, 11]\n",
        "    mapping_mse_full = [0, 3, 6, 9, 12]\n",
        "\n",
        "    student_model.train()\n",
        "    teacher_model.eval()\n",
        "\n",
        "    for epoch in range(wandb.config.epochs):\n",
        "        total_loss = 0\n",
        "        total_task_loss = 0\n",
        "        total_kl_loss = 0\n",
        "        total_hs_loss = 0\n",
        "        total_attn_loss = 0\n",
        "\n",
        "        pb = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\")\n",
        "        for batch in pb:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                teacher_outputs = teacher_model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    output_hidden_states=True,\n",
        "                    output_attentions=True\n",
        "                )\n",
        "            teacher_logits = teacher_outputs.logits\n",
        "            teacher_hiddens = teacher_outputs.hidden_states\n",
        "            teacher_attns = teacher_outputs.attentions\n",
        "\n",
        "            with autocast('cuda', dtype=torch.float16):\n",
        "                student_outputs = student_model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    return_hidden_states=True,\n",
        "                    return_attentions=True\n",
        "                )\n",
        "                student_logits = student_outputs[\"logits\"]\n",
        "                student_hiddens = student_outputs[\"hidden_states\"]\n",
        "                student_attns = student_outputs[\"attentions\"]\n",
        "\n",
        "                task_loss = F.cross_entropy(student_logits, labels)\n",
        "                kl_loss = kl_div_loss(student_logits, teacher_logits, temperature=wandb.config.temperature)\n",
        "\n",
        "                hs_loss = 0\n",
        "                for i, student_hs in enumerate(student_hiddens):\n",
        "                    teacher_hs = teacher_hiddens[mapping_mse_full[i]]\n",
        "                    hs_loss += hidden_state_loss(student_hs, teacher_hs)\n",
        "\n",
        "                attn_loss = attention_kl_loss(\n",
        "                    student_attns,\n",
        "                    teacher_attns,\n",
        "                    mapping_attn,\n",
        "                    temperature=wandb.config.temperature\n",
        "                )\n",
        "\n",
        "                total_loss = (\n",
        "                    wandb.config.a * task_loss +\n",
        "                    wandb.config.b * kl_loss +\n",
        "                    wandb.config.c * hs_loss +\n",
        "                    wandb.config.d * attn_loss\n",
        "                )\n",
        "\n",
        "            scaler.scale(total_loss).backward()\n",
        "            torch.nn.utils.clip_grad_norm_(student_model.parameters(), 1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            total_loss_value = total_loss.item()\n",
        "            total_task_loss += task_loss.item()\n",
        "            total_kl_loss += kl_loss.item()\n",
        "            total_hs_loss += hs_loss.item()\n",
        "            total_attn_loss += attn_loss.item()\n",
        "\n",
        "            pb.set_postfix(\n",
        "                loss=total_loss_value,\n",
        "                task=task_loss.item(),\n",
        "                kl=kl_loss.item(),\n",
        "                hs=hs_loss.item(),\n",
        "                attn=attn_loss.item()\n",
        "            )\n",
        "\n",
        "            wandb.log({\n",
        "                \"step\": step + 1,\n",
        "                \"train_loss\": total_loss_value,\n",
        "                \"task_loss\": task_loss.item(),\n",
        "                \"kl_loss\": kl_loss.item(),\n",
        "                \"hs_loss\": hs_loss.item(),\n",
        "                \"attn_loss\": attn_loss.item()\n",
        "            })\n",
        "\n",
        "            step += 1\n",
        "\n",
        "        student_model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "\n",
        "                outputs = student_model(input_ids, attention_mask=attention_mask)\n",
        "                logits = outputs[\"logits\"] if isinstance(outputs, dict) else outputs\n",
        "                predictions = torch.argmax(logits, dim=1)\n",
        "                correct += (predictions == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "        val_acc = correct / total\n",
        "        wandb.log({\"epoch\": epoch + 1, \"val_acc\": val_acc})\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(student_model.state_dict(), \"best_distilled_model.pt\")\n",
        "\n",
        "            state_dict = {}\n",
        "            for k, v in student_model.state_dict().items():\n",
        "                state_dict[k] = v.clone().half()\n",
        "\n",
        "            torch.save(state_dict, \"best_distilled_model_fp16.pt\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | Val Acc: {val_acc * 100:.2f}%\\n\")\n",
        "\n",
        "    return student_model"
      ],
      "metadata": {
        "id": "JzQfNBg3y7ax"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_distil_model(distil_model, bert_model, tokenizer, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 987
        },
        "id": "eFL2lDOT1Ywn",
        "outputId": "399c12f8-5bcf-417d-e817-2c4af90a87a9"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 459/459 [02:52<00:00,  2.66it/s, attn=1.23, hs=2.67, kl=0.256, loss=2.45, task=0.196]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Val Acc: 70.43%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 459/459 [02:47<00:00,  2.74it/s, attn=0.902, hs=2.09, kl=0.728, loss=3.74, task=0.484]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Val Acc: 62.43%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 459/459 [02:47<00:00,  2.74it/s, attn=0.808, hs=1.95, kl=0.744, loss=4.25, task=0.63]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Val Acc: 67.54%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 459/459 [02:47<00:00,  2.74it/s, attn=0.76, hs=1.83, kl=0.991, loss=3.7, task=0.445]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Val Acc: 62.72%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 459/459 [02:47<00:00,  2.74it/s, attn=0.805, hs=1.88, kl=1.21, loss=4.12, task=0.486]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Val Acc: 68.58%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 459/459 [02:47<00:00,  2.74it/s, attn=0.707, hs=1.88, kl=0.707, loss=2.85, task=0.305]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 | Val Acc: 63.71%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 459/459 [02:47<00:00,  2.74it/s, attn=0.784, hs=1.86, kl=0.812, loss=2.41, task=0.163]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 | Val Acc: 62.72%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 459/459 [02:47<00:00,  2.74it/s, attn=0.816, hs=1.89, kl=0.26, loss=2.65, task=0.353]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 | Val Acc: 58.20%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 459/459 [02:47<00:00,  2.74it/s, attn=0.793, hs=1.84, kl=0.881, loss=2.17, task=0.0848]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 | Val Acc: 63.36%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 459/459 [02:48<00:00,  2.73it/s, attn=0.864, hs=1.81, kl=0.303, loss=1.95, task=0.169]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 | Val Acc: 61.68%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|██████████| 459/459 [02:47<00:00,  2.74it/s, attn=0.753, hs=1.77, kl=0.0176, loss=0.99, task=0.0165]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 | Val Acc: 61.86%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|██████████| 459/459 [02:47<00:00,  2.74it/s, attn=0.748, hs=1.71, kl=0.421, loss=1.7, task=0.0983]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 | Val Acc: 60.70%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13:   9%|▊         | 40/459 [00:14<02:36,  2.67it/s, attn=0.825, hs=1.8, kl=0.367, loss=1.59, task=0.0675]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-828078ebb88a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_distil_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistil_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-76-34fcbfbe3161>\u001b[0m in \u001b[0;36mtrain_distil_model\u001b[0;34m(student_model, teacher_model, tokenizer, device)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opt_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[union-attr]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped_by_lr_sched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m                             )\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    241\u001b[0m             )\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             adamw(\n\u001b[0m\u001b[1;32m    244\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adamw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    876\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m_multi_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         torch._foreach_addcmul_(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mdevice_exp_avg_sqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaled_device_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тут такой прикол, что в fp16 я не стал обучать явно, а через автокаст, который только градиенты с некоторых (почти всех) слоев считает в fp16, веса хранит в fp32. По идее после torch.half() точность не упадет (у меня не падала). Я так сделал, потому что карпатый, вроде как, тоже в автокасте обучал. А в half() я никого не видел."
      ],
      "metadata": {
        "id": "8LMrXpy_2ok6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distil_model.load_state_dict(torch.load(f\"best_distilled_model_fp16.pt\"))\n",
        "distil_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "train_loader, val_loader = prepare_data(batch_size=wandb.config.batch_size)\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(val_loader):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = distil_model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs[\"logits\"] if isinstance(outputs, dict) else outputs\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "        correct += (predictions == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "print(\"\\n\", correct / total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2KDN_-thHFM",
        "outputId": "e4119612-bea8-4b44-99d5-c768bde2c893"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 216/216 [00:10<00:00, 19.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 0.7049275362318841\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "У меня максимально выбивает 0.7. После всех игр с лоссами. Нестабильность в vall_accuracy, как будто бы, из-за того, что модель инстантно переучиваться начинает. Ну или, может быть, из-за нестабильности в важности лоссов. Хотя я попытался это минимизировать, навряд ли из-за этого.\n",
        "\n",
        "Короче как будто бы я в хард кап упираюсь вычислительных способностей берта с 4 слоями и fp16 Точностью"
      ],
      "metadata": {
        "id": "p71j00ECRxBf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Часть 2. Дистиляция с шумом, в fp16."
      ],
      "metadata": {
        "id": "FKFDfwLpkPXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Тут я обучаю в fp16 с шумом.**"
      ],
      "metadata": {
        "id": "5LppdbXkvMui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_noise(tensor, noise_std=0.05):\n",
        "    if tensor is None:\n",
        "        return None\n",
        "    if noise_std > 0.0 and tensor.requires_grad:\n",
        "        noise = torch.randn_like(tensor) * noise_std\n",
        "        return tensor + noise\n",
        "    return tensor\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.mha = nn.MultiheadAttention(\n",
        "            embed_dim=config.bert_hidden_size,\n",
        "            num_heads=config.num_heads,\n",
        "            dropout=config.dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(config.bert_hidden_size, config.intermediate_size),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(config.intermediate_size, config.bert_hidden_size)\n",
        "        )\n",
        "\n",
        "        self.layernorm1 = nn.LayerNorm(config.bert_hidden_size, eps=1e-12)\n",
        "        self.layernorm2 = nn.LayerNorm(config.bert_hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x, attention_mask=None, return_attention=False):\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = ~attention_mask.bool()\n",
        "        else:\n",
        "            attention_mask = None\n",
        "\n",
        "        mha_out, attn_weights = self.mha(\n",
        "            query=x,\n",
        "            key=x,\n",
        "            value=x,\n",
        "            key_padding_mask=attention_mask,\n",
        "            need_weights=return_attention,\n",
        "            average_attn_weights=False\n",
        "        )\n",
        "\n",
        "        residual = x + self.dropout(mha_out)\n",
        "        x = self.layernorm1(residual)\n",
        "\n",
        "        ffn_out = self.ffn(x)\n",
        "        residual = x + self.dropout(ffn_out)\n",
        "        output = self.layernorm2(residual)\n",
        "\n",
        "        if return_attention:\n",
        "            return output, attn_weights\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "class CleanBERT(nn.Module):\n",
        "    def __init__(self, embedding_layer, config, num_labels=2):\n",
        "        super().__init__()\n",
        "        self.embeddings = embedding_layer\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(config) for _ in range(config.num_blocks)\n",
        "        ])\n",
        "\n",
        "        self.pooler = nn.Sequential(\n",
        "            nn.Linear(config.bert_hidden_size, config.bert_hidden_size),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.classifier = nn.Linear(config.bert_hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None,\n",
        "                return_hidden_states=False, return_attentions=False):\n",
        "        x = self.embeddings(input_ids)\n",
        "\n",
        "        if self.training:\n",
        "            x = add_noise(x, noise_std=0.05)\n",
        "\n",
        "        all_hidden_states = []\n",
        "        all_attentions = []\n",
        "\n",
        "        if return_hidden_states:\n",
        "            all_hidden_states.append(x)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            if return_attentions:\n",
        "                x, attn = block(x, attention_mask=attention_mask, return_attention=True)\n",
        "                all_attentions.append(attn)\n",
        "            else:\n",
        "                x = block(x, attention_mask=attention_mask, return_attention=False)\n",
        "\n",
        "            if self.training:\n",
        "                x = add_noise(x, noise_std=0.05)\n",
        "\n",
        "            if return_hidden_states:\n",
        "                all_hidden_states.append(x)\n",
        "\n",
        "        cls_output = x[:, 0]\n",
        "\n",
        "        pooled = self.pooler(cls_output)\n",
        "        if self.training:\n",
        "            pooled = add_noise(pooled, noise_std=0.05)\n",
        "        pooled = self.dropout(pooled)\n",
        "        logits = self.classifier(pooled)\n",
        "\n",
        "        outputs = {\"logits\": logits}\n",
        "\n",
        "        if return_hidden_states:\n",
        "            outputs[\"hidden_states\"] = all_hidden_states\n",
        "\n",
        "        if return_attentions:\n",
        "            outputs[\"attentions\"] = all_attentions\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "Vk6flaUDi-JS"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"bert_mrpc_noice\", config={\n",
        "    \"a\": 4,\n",
        "    \"b\": 1,\n",
        "    \"c\": 0.3,\n",
        "    \"d\": 0.5,\n",
        "    \"batch_size\": 8,\n",
        "    \"epochs\": 13,\n",
        "    \"lr\": 6e-5,\n",
        "    \"temperature\": 1.25\n",
        "})"
      ],
      "metadata": {
        "id": "LKlOV7uNmL8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distil_model = CleanBERT(copy.deepcopy(bert_model.bert.embeddings), config).to(device)\n",
        "for param in distil_model.embeddings.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "train_distil_model(distil_model, bert_model, tokenizer, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 950
        },
        "id": "76zojYLWl-Q0",
        "outputId": "829e3413-4d25-4663-90c1-e0bbadc02adb"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 459/459 [02:50<00:00,  2.69it/s, attn=1.17, hs=2.75, kl=1.18, loss=5.25, task=0.667]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Val Acc: 70.03%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 459/459 [02:47<00:00,  2.74it/s, attn=0.955, hs=2.16, kl=0.649, loss=3.58, task=0.451]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Val Acc: 69.33%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 459/459 [02:47<00:00,  2.74it/s, attn=0.847, hs=1.95, kl=0.789, loss=4.83, task=0.759]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Val Acc: 59.48%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 459/459 [02:47<00:00,  2.74it/s, attn=0.862, hs=2.1, kl=0.469, loss=2.92, task=0.347]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Val Acc: 63.01%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 459/459 [02:47<00:00,  2.74it/s, attn=0.806, hs=1.92, kl=1.07, loss=4.85, task=0.701]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Val Acc: 66.20%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 459/459 [02:47<00:00,  2.74it/s, attn=0.821, hs=1.91, kl=1.01, loss=3.81, task=0.455]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 | Val Acc: 65.45%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 459/459 [02:47<00:00,  2.74it/s, attn=0.839, hs=1.83, kl=1.43, loss=3.36, task=0.24]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 | Val Acc: 66.14%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 459/459 [02:47<00:00,  2.74it/s, attn=0.851, hs=1.98, kl=0.682, loss=2.86, task=0.289]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 | Val Acc: 64.99%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 459/459 [02:47<00:00,  2.74it/s, attn=0.823, hs=1.95, kl=0.677, loss=3.95, task=0.569]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 | Val Acc: 60.64%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 459/459 [02:47<00:00,  2.74it/s, attn=0.824, hs=1.89, kl=0.758, loss=3.52, task=0.447]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 | Val Acc: 64.35%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|██████████| 459/459 [02:47<00:00,  2.74it/s, attn=0.779, hs=1.83, kl=0.203, loss=1.44, task=0.0746]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 | Val Acc: 63.54%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12:  21%|██        | 95/459 [00:35<02:14,  2.70it/s, attn=0.767, hs=1.91, kl=0.483, loss=1.75, task=0.0792]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-dbbab1938323>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_distil_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistil_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-76-34fcbfbe3161>\u001b[0m in \u001b[0;36mtrain_distil_model\u001b[0;34m(student_model, teacher_model, tokenizer, device)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "distil_model.load_state_dict(torch.load(f\"best_distilled_model_fp16.pt\"))\n",
        "distil_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "train_loader, val_loader = prepare_data(batch_size=wandb.config.batch_size)\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(val_loader):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = distil_model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs[\"logits\"] if isinstance(outputs, dict) else outputs\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "        correct += (predictions == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "print(\"\\n\", correct / total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_OUiv0DmWWZ",
        "outputId": "939eeef5-c4de-4e2a-d8cf-b4e0cd47b3bd"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 216/216 [00:10<00:00, 19.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 0.7002898550724638\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Часть 3. Дистиляция с шумом, в fp32."
      ],
      "metadata": {
        "id": "eObOK3urvxoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"bert_mrpc_noice\", config={\n",
        "    \"a\": 4,\n",
        "    \"b\": 1,\n",
        "    \"c\": 0.3,\n",
        "    \"d\": 0.5,\n",
        "    \"batch_size\": 8,\n",
        "    \"epochs\": 13,\n",
        "    \"lr\": 6e-5,\n",
        "    \"temperature\": 1.25\n",
        "})"
      ],
      "metadata": {
        "id": "Tt8rwAKMv0qE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_distil_model_2(student_model, teacher_model, tokenizer, device):\n",
        "    train_loader, val_loader = prepare_data(batch_size=wandb.config.batch_size)\n",
        "\n",
        "    optimizer = AdamW(student_model.parameters(), lr=wandb.config.lr)\n",
        "    total_steps = len(train_loader) * wandb.config.epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=100, num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    step = 0\n",
        "\n",
        "    scaler = GradScaler('cuda')\n",
        "\n",
        "    mapping_attn = [2, 5, 8, 11]\n",
        "    mapping_mse_full = [0, 3, 6, 9, 12]\n",
        "\n",
        "    student_model.train()\n",
        "    teacher_model.eval()\n",
        "\n",
        "    for epoch in range(wandb.config.epochs):\n",
        "        total_loss = 0\n",
        "        total_task_loss = 0\n",
        "        total_kl_loss = 0\n",
        "        total_hs_loss = 0\n",
        "        total_attn_loss = 0\n",
        "\n",
        "        pb = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\")\n",
        "        for batch in pb:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                teacher_outputs = teacher_model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    output_hidden_states=True,\n",
        "                    output_attentions=True\n",
        "                )\n",
        "            teacher_logits = teacher_outputs.logits\n",
        "            teacher_hiddens = teacher_outputs.hidden_states\n",
        "            teacher_attns = teacher_outputs.attentions\n",
        "\n",
        "            student_outputs = student_model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                return_hidden_states=True,\n",
        "                return_attentions=True\n",
        "            )\n",
        "            student_logits = student_outputs[\"logits\"]\n",
        "            student_hiddens = student_outputs[\"hidden_states\"]\n",
        "            student_attns = student_outputs[\"attentions\"]\n",
        "\n",
        "            task_loss = F.cross_entropy(student_logits, labels)\n",
        "            kl_loss = kl_div_loss(student_logits, teacher_logits, temperature=wandb.config.temperature)\n",
        "\n",
        "            hs_loss = 0\n",
        "            for i, student_hs in enumerate(student_hiddens):\n",
        "                teacher_hs = teacher_hiddens[mapping_mse_full[i]]\n",
        "                hs_loss += hidden_state_loss(student_hs, teacher_hs)\n",
        "\n",
        "            attn_loss = attention_kl_loss(\n",
        "                student_attns,\n",
        "                teacher_attns,\n",
        "                mapping_attn,\n",
        "                temperature=wandb.config.temperature\n",
        "            )\n",
        "\n",
        "            total_loss = (\n",
        "                wandb.config.a * task_loss +\n",
        "                wandb.config.b * kl_loss +\n",
        "                wandb.config.c * hs_loss +\n",
        "                wandb.config.d * attn_loss\n",
        "            )\n",
        "\n",
        "            scaler.scale(total_loss).backward()\n",
        "            torch.nn.utils.clip_grad_norm_(student_model.parameters(), 1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            total_loss_value = total_loss.item()\n",
        "            total_task_loss += task_loss.item()\n",
        "            total_kl_loss += kl_loss.item()\n",
        "            total_hs_loss += hs_loss.item()\n",
        "            total_attn_loss += attn_loss.item()\n",
        "\n",
        "            pb.set_postfix(\n",
        "                loss=total_loss_value,\n",
        "                task=task_loss.item(),\n",
        "                kl=kl_loss.item(),\n",
        "                hs=hs_loss.item(),\n",
        "                attn=attn_loss.item()\n",
        "            )\n",
        "\n",
        "            wandb.log({\n",
        "                \"step\": step + 1,\n",
        "                \"train_loss\": total_loss_value,\n",
        "                \"task_loss\": task_loss.item(),\n",
        "                \"kl_loss\": kl_loss.item(),\n",
        "                \"hs_loss\": hs_loss.item(),\n",
        "                \"attn_loss\": attn_loss.item()\n",
        "            })\n",
        "\n",
        "            step += 1\n",
        "\n",
        "        student_model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "\n",
        "                outputs = student_model(input_ids, attention_mask=attention_mask)\n",
        "                logits = outputs[\"logits\"] if isinstance(outputs, dict) else outputs\n",
        "                predictions = torch.argmax(logits, dim=1)\n",
        "                correct += (predictions == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "        val_acc = correct / total\n",
        "        wandb.log({\"epoch\": epoch + 1, \"val_acc\": val_acc})\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(student_model.state_dict(), \"best_distilled_model.pt\")\n",
        "\n",
        "            state_dict = {}\n",
        "            for k, v in student_model.state_dict().items():\n",
        "                state_dict[k] = v.clone().half()\n",
        "\n",
        "            torch.save(state_dict, \"best_distilled_model_fp16.pt\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | Val Acc: {val_acc * 100:.2f}%\\n\")\n",
        "\n",
        "    return student_model"
      ],
      "metadata": {
        "id": "EYUKn49Uv4ee"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distil_model = CleanBERT(copy.deepcopy(bert_model.bert.embeddings), config).to(device)\n",
        "for param in distil_model.embeddings.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "train_distil_model_2(distil_model, bert_model, tokenizer, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569
        },
        "id": "TcncUbWQwRF8",
        "outputId": "8f96c0d8-b434-4d08-d60c-60c3f9f5e188"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 459/459 [03:30<00:00,  2.18it/s, attn=1.54, hs=3.66, kl=0.665, loss=6.31, task=0.944]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Val Acc: 65.10%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 459/459 [03:24<00:00,  2.25it/s, attn=1.11, hs=2.57, kl=0.815, loss=5.34, task=0.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Val Acc: 64.81%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 459/459 [03:24<00:00,  2.25it/s, attn=1.15, hs=2.29, kl=0.561, loss=3.73, task=0.477]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Val Acc: 57.04%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 459/459 [03:24<00:00,  2.25it/s, attn=1.03, hs=2.23, kl=0.49, loss=3.3, task=0.407]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Val Acc: 64.58%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5:  31%|███       | 141/459 [01:02<02:21,  2.25it/s, attn=1.07, hs=2.17, kl=0.667, loss=4.07, task=0.554]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-9a2136361a71>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_distil_model_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistil_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-84-ea65da940203>\u001b[0m in \u001b[0;36mtrain_distil_model_2\u001b[0;34m(student_model, teacher_model, tokenizer, device)\u001b[0m\n\u001b[1;32m     75\u001b[0m             )\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "distil_model.load_state_dict(torch.load(f\"best_distilled_model_fp16.pt\"))\n",
        "distil_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "train_loader, val_loader = prepare_data(batch_size=wandb.config.batch_size)\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(val_loader):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = distil_model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs[\"logits\"] if isinstance(outputs, dict) else outputs\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "        correct += (predictions == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "print(\"\\n\", correct / total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnUQJt0swSKE",
        "outputId": "c9ae621c-f013-4fb2-b865-ec0d19db3320"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 216/216 [00:10<00:00, 19.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 0.6504347826086957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Вроде как шум спас от потери точности при конвертации fp32 -> fp16, но, походу, зашумленные fp32 градиенты хуже справляются со своей задачей, нежели зашумленные fp16**"
      ],
      "metadata": {
        "id": "V3SAYy0o0Oxz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Часть 4. Просто дистиляция и torch.half()"
      ],
      "metadata": {
        "id": "AAau3YgJ0xR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"bert_mrpc_part4\", config={\n",
        "    \"a\": 4,\n",
        "    \"b\": 1,\n",
        "    \"c\": 0.3,\n",
        "    \"d\": 0.5,\n",
        "    \"batch_size\": 8,\n",
        "    \"epochs\": 13,\n",
        "    \"lr\": 6e-5,\n",
        "    \"temperature\": 1.25\n",
        "})"
      ],
      "metadata": {
        "id": "TT0psVqc0_wR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.mha = nn.MultiheadAttention(\n",
        "            embed_dim=config.bert_hidden_size,\n",
        "            num_heads=config.num_heads,\n",
        "            dropout=config.dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(config.bert_hidden_size, config.intermediate_size),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(config.intermediate_size, config.bert_hidden_size)\n",
        "        )\n",
        "\n",
        "        self.layernorm1 = nn.LayerNorm(config.bert_hidden_size, eps=1e-12)\n",
        "        self.layernorm2 = nn.LayerNorm(config.bert_hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x, attention_mask=None, return_attention=False):\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = ~attention_mask.bool()\n",
        "        else:\n",
        "            attention_mask = None\n",
        "\n",
        "        mha_out, attn_weights = self.mha(\n",
        "            query=x,\n",
        "            key=x,\n",
        "            value=x,\n",
        "            key_padding_mask=attention_mask,\n",
        "            need_weights=return_attention,\n",
        "            average_attn_weights=False\n",
        "        )\n",
        "\n",
        "        residual = x + self.dropout(mha_out)\n",
        "        x = self.layernorm1(residual)\n",
        "\n",
        "        ffn_out = self.ffn(x)\n",
        "        residual = x + self.dropout(ffn_out)\n",
        "        output = self.layernorm2(residual)\n",
        "\n",
        "        if return_attention:\n",
        "            return output, attn_weights\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "class CleanBERT(nn.Module):\n",
        "    def __init__(self, embedding_layer, config, num_labels=2):\n",
        "        super().__init__()\n",
        "        self.embeddings = embedding_layer\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(config) for _ in range(config.num_blocks)\n",
        "        ])\n",
        "\n",
        "        self.pooler = nn.Sequential(\n",
        "            nn.Linear(config.bert_hidden_size, config.bert_hidden_size),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.classifier = nn.Linear(config.bert_hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None,\n",
        "                return_hidden_states=False, return_attentions=False):\n",
        "        x = self.embeddings(input_ids)\n",
        "\n",
        "        all_hidden_states = []\n",
        "        all_attentions = []\n",
        "\n",
        "        if return_hidden_states:\n",
        "            all_hidden_states.append(x)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            if return_attentions:\n",
        "                x, attn = block(x, attention_mask=attention_mask, return_attention=True)\n",
        "                all_attentions.append(attn)\n",
        "            else:\n",
        "                x = block(x, attention_mask=attention_mask, return_attention=False)\n",
        "\n",
        "            if return_hidden_states:\n",
        "                all_hidden_states.append(x)\n",
        "\n",
        "        cls_output = x[:, 0]\n",
        "\n",
        "        pooled = self.pooler(cls_output)\n",
        "        pooled = self.dropout(pooled)\n",
        "        logits = self.classifier(pooled)\n",
        "\n",
        "        outputs = {\"logits\": logits}\n",
        "\n",
        "        if return_hidden_states:\n",
        "            outputs[\"hidden_states\"] = all_hidden_states\n",
        "\n",
        "        if return_attentions:\n",
        "            outputs[\"attentions\"] = all_attentions\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "J7rOYgJw1O5x"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distil_model = CleanBERT(copy.deepcopy(bert_model.bert.embeddings), config).to(device)\n",
        "for param in distil_model.embeddings.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "train_distil_model_2(distil_model, bert_model, tokenizer, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569
        },
        "id": "7iPUFvpL1P7X",
        "outputId": "3d3dbc20-b10c-411f-f87c-b526e667e572"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 459/459 [03:26<00:00,  2.22it/s, attn=1.43, hs=3.56, kl=1.07, loss=5.5, task=0.662]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Val Acc: 68.23%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 459/459 [03:24<00:00,  2.25it/s, attn=1.11, hs=2.5, kl=1.03, loss=4.8, task=0.617]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Val Acc: 64.70%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 459/459 [03:24<00:00,  2.25it/s, attn=1.05, hs=2.35, kl=0.236, loss=3.3, task=0.459]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Val Acc: 62.96%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 459/459 [03:24<00:00,  2.25it/s, attn=1.07, hs=2.2, kl=0.559, loss=3.95, task=0.549]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Val Acc: 64.81%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5:  40%|████      | 184/459 [01:22<02:03,  2.23it/s, attn=1.1, hs=2.2, kl=0.411, loss=3.55, task=0.484]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-9a2136361a71>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_distil_model_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistil_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-84-ea65da940203>\u001b[0m in \u001b[0;36mtrain_distil_model_2\u001b[0;34m(student_model, teacher_model, tokenizer, device)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "distil_model.load_state_dict(torch.load(f\"best_distilled_model_fp16.pt\"))\n",
        "distil_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "train_loader, val_loader = prepare_data(batch_size=wandb.config.batch_size)\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(val_loader):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = distil_model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs[\"logits\"] if isinstance(outputs, dict) else outputs\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "        correct += (predictions == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "print(\"\\n\", correct / total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKPCmg5W1V-B",
        "outputId": "1e4bd41f-4178-4da2-9fb6-f27dcbadf61f"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 216/216 [00:10<00:00, 19.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 0.6823188405797102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}
