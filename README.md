# cleanLLM
cleanLLM is a showcase of all known methods of large language model distillation.
___
# Что делаем:

## База
1. Накодить модель ученика (LSTM, LSTM + Transformer, Transformer)  (BERT)
2. Визуал attention-ов
3. Обучить с шумом для квантования  (1. Дистиллируем в меньшей точности 2. Дистиллируем с шумом -> загоняем в меньшую точность 3. Дистиллируем -> загоняем в меньшую точность, сравниваем метрики)  (За основу возьмём BERT Transformer 4 слоя, датасет MRPC из GLUE)
4. Квантовать + прунинг (1. Дистиллировать, потом квантовать посмотреть на точность. 2. Дистиллировать, потмо запрунить. Посмотреть на точность. 3. Дистиллировать, потом квантануть + прунить, посмотреть на точность. 4. Запрунить модель до такого же количества весов и посмотреть на разницу.) fp16, i8
5. Несколько техник
## Дополнительно:

1. Обучить RL-ем
2. Несколько учителей (BERT на нескольких корпусах или BERT + GPT)
3. Самодистиляция
4. Обучить без меток

## Полезные ссылочки:
1. [Дистиляция Берта](https://github.com/elephantmipt/bert-distillation)
2. [Математика в дистиляции](https://leeyngdo.github.io/blog/deep-learning/2024-01-21-knowledge-distillation/?utm_source=chatgpt.com)
