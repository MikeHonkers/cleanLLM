# cleanLLM
cleanLLM is a showcase of all known methods of large language model distillation.
___
# Что делаем:

## База
1. Накодить модель ученика (LSTM, LSTM + Transformer, Transformer)  (BERT)
2. Визуал attention-ов
3. Обучить с шумом для квантования
4. Квантовать + прунинг
5. Несколько техник
## Дополнительно:

1. Обучить RL-ем
2. Несколько учителей (BERT на нескольких корпусах или BERT + GPT)
3. Самодистиляция

## Полезные ссылочки:
1. [Дистиляция Берта](https://github.com/elephantmipt/bert-distillation)
