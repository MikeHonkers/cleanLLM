{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets -q"
      ],
      "metadata": {
        "id": "ca9vPWZ6cjBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ALmkUTRI2q5J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from dataclasses import dataclass\n",
        "import copy\n",
        "import wandb\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from datasets import load_dataset\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('Intel/bert-base-uncased-mrpc')\n",
        "distil_model = AutoModelForSequenceClassification.from_pretrained(\"Intel/bert-base-uncased-mrpc\").to(device)\n",
        "distil_model.eval()"
      ],
      "metadata": {
        "id": "WQXjmFW37J3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) EMA обновление весов\n",
        "\n",
        "2) Loss = a * task_loss (CrossEntropy) + b * logits_loss (KLDiv) + c * attention_loss (KLDiv)"
      ],
      "metadata": {
        "id": "gNp8_NcHNpfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(batch_size=16):\n",
        "    dataset = load_dataset(\"nyu-mll/glue\", \"mrpc\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"Intel/bert-base-uncased-mrpc\")\n",
        "\n",
        "    def tokenize(batch):\n",
        "        return tokenizer(\n",
        "            text=batch[\"sentence1\"],\n",
        "            text_pair=batch[\"sentence2\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "    encoded_dataset = dataset.map(tokenize, batched=True, batch_size=batch_size)\n",
        "    encoded_dataset = encoded_dataset.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
        "    encoded_dataset = encoded_dataset.rename_column(\"label\", \"labels\")\n",
        "    encoded_dataset.set_format(\"torch\")\n",
        "\n",
        "    train_loader = DataLoader(encoded_dataset[\"train\"], batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(encoded_dataset[\"test\"], batch_size=batch_size)\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "def kl_div_loss(student_logits, teacher_logits, temperature=2.3):\n",
        "    student_log_probs = F.log_softmax(student_logits / temperature, dim=1)\n",
        "    teacher_probs = F.softmax(teacher_logits / temperature, dim=1)\n",
        "    return F.kl_div(student_log_probs, teacher_probs, reduction='batchmean') * (temperature ** 2)\n",
        "\n",
        "def attention_kl_loss(student_attns, teacher_attns, mapping_attn, temperature=2.3):\n",
        "    total_loss = 0.0\n",
        "    num_layers = len(student_attns)\n",
        "\n",
        "    for student_idx in range(num_layers):\n",
        "        student_attn = student_attns[student_idx]  # [B, H, T, T]\n",
        "        teacher_attn = teacher_attns[mapping_attn[student_idx]]  # [B, H, T, T]\n",
        "\n",
        "        B, H, T, _ = student_attn.shape\n",
        "\n",
        "        # Применяем температурное масштабирование\n",
        "        student_attn = student_attn / temperature\n",
        "        teacher_attn = teacher_attn / temperature\n",
        "\n",
        "        # Вычисляем логарифмы вероятностей и вероятности\n",
        "        student_log_probs = F.log_softmax(student_attn, dim=-1)  # [B, H, T, T]\n",
        "        teacher_probs = F.softmax(teacher_attn, dim=-1)  # [B, H, T, T]\n",
        "\n",
        "        # Вычисляем KL-дивергенцию для каждого элемента\n",
        "        kl_per_element = F.kl_div(\n",
        "            student_log_probs,\n",
        "            teacher_probs,\n",
        "            reduction='none'\n",
        "        )  # [B, H, T, T]\n",
        "\n",
        "        # Суммируем по последнему измерению (T)\n",
        "        kl_per_token = kl_per_element.sum(dim=-1)  # [B, H, T]\n",
        "\n",
        "        # Суммируем по всем головам и токенам (но не по батчу!)\n",
        "        kl_per_layer = kl_per_token.sum(dim=(1, 2))  # [B]\n",
        "\n",
        "        # Усредняем по батчу\n",
        "        layer_loss = kl_per_layer.mean()\n",
        "\n",
        "        # Масштабируем обратно температурой\n",
        "        layer_loss = layer_loss * (temperature ** 2)\n",
        "\n",
        "        total_loss += layer_loss\n",
        "\n",
        "    return total_loss / num_layers"
      ],
      "metadata": {
        "id": "V51YfkU0ULrY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EMAModel:\n",
        "    def __init__(self, model, decay=0.99):\n",
        "        self.decay = decay\n",
        "        self.shadow = copy.deepcopy(model)\n",
        "        self.shadow.eval()\n",
        "\n",
        "    def update(self, model):\n",
        "        with torch.no_grad():\n",
        "            for shadow_param, model_param in zip(self.shadow.parameters(), model.parameters()):\n",
        "                shadow_param.copy_(shadow_param * self.decay + (1 - self.decay) * model_param)"
      ],
      "metadata": {
        "id": "jmVyMlpruBZ2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"bert_mrpc_self-distil\", config={\n",
        "    \"a\": 2,\n",
        "    \"b\": 1,\n",
        "    \"c\": 1,\n",
        "    \"batch_size\": 8,\n",
        "    \"epochs\": 13,\n",
        "    \"lr\": 6e-5,\n",
        "    \"temperature\": 1.5\n",
        "})"
      ],
      "metadata": {
        "id": "6rmCES1osH59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_teacher(model, tokenizer, device):\n",
        "    train_loader, val_loader = prepare_data(batch_size=wandb.config.batch_size)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=wandb.config.lr)\n",
        "    total_steps = len(train_loader) * wandb.config.epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=100, num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    step = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(wandb.config.epochs):\n",
        "        epoch_task_loss = 0\n",
        "\n",
        "        pb = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\")\n",
        "        for batch in pb:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            student_outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "            student_logits = student_outputs.logits\n",
        "\n",
        "            task_loss = F.cross_entropy(student_logits, labels)\n",
        "\n",
        "            total_loss = task_loss\n",
        "\n",
        "            total_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        train_loader, val_loader = prepare_data(batch_size=wandb.config.batch_size)\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader):\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                preds = torch.argmax(outputs.logits, dim=1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "        print(\"\\n\", correct / total)\n",
        "\n",
        "        step += 1\n",
        "\n",
        "train_teacher(distil_model, tokenizer, 'cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "XmNp0lJn_FvS",
        "outputId": "f2cf01dd-01d6-4a1e-a75c-676a4f68f50d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 459/459 [05:41<00:00,  1.34it/s]\n",
            "100%|██████████| 216/216 [00:50<00:00,  4.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 0.8057971014492754\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 459/459 [05:33<00:00,  1.37it/s]\n",
            "100%|██████████| 216/216 [00:50<00:00,  4.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 0.7831884057971015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3:  12%|█▏        | 56/459 [00:40<04:53,  1.37it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-7ffa135b8cc1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mtrain_teacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistil_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-7ffa135b8cc1>\u001b[0m in \u001b[0;36mtrain_teacher\u001b[0;34m(model, tokenizer, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mpb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch + 1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_dropout_and_norm_to_eval(module):\n",
        "    if isinstance(module, torch.nn.modules.dropout._DropoutNd):\n",
        "        module.eval()\n",
        "    elif isinstance(module, torch.nn.modules.batchnorm._BatchNorm):\n",
        "        module.eval()\n",
        "    elif isinstance(module, torch.nn.LayerNorm):\n",
        "        module.eval()\n",
        "\n",
        "    for child in module.children():\n",
        "        set_dropout_and_norm_to_eval(child)"
      ],
      "metadata": {
        "id": "PNBj2f8qEfIX"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_distil_model(student_model, tokenizer, device):\n",
        "    train_loader, val_loader = prepare_data(batch_size=wandb.config.batch_size)\n",
        "\n",
        "    optimizer = AdamW(student_model.parameters(), lr=wandb.config.lr)\n",
        "    total_steps = len(train_loader) * wandb.config.epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=100, num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    ema_teacher = EMAModel(student_model, decay=0.99)\n",
        "    best_val_acc = 0.0\n",
        "    step = 0\n",
        "\n",
        "    mapping_attn = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
        "\n",
        "    student_model.train()\n",
        "    set_dropout_and_norm_to_eval(student_model)\n",
        "\n",
        "    for epoch in range(wandb.config.epochs):\n",
        "        epoch_task_loss = 0\n",
        "        epoch_kl_loss = 0\n",
        "        epoch_attn_loss = 0\n",
        "        epoch_total_loss = 0\n",
        "\n",
        "        pb = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\")\n",
        "        for batch in pb:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                teacher_outputs = ema_teacher.shadow(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    output_attentions=True\n",
        "                )\n",
        "            teacher_logits = teacher_outputs.logits\n",
        "            teacher_attns = teacher_outputs.attentions\n",
        "\n",
        "            student_outputs = student_model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                output_attentions=True\n",
        "            )\n",
        "            student_logits = student_outputs.logits\n",
        "            student_attns = student_outputs.attentions\n",
        "\n",
        "            task_loss = F.cross_entropy(student_logits, labels)\n",
        "            kl_loss = kl_div_loss(student_logits, teacher_logits, temperature=wandb.config.temperature)\n",
        "            attn_loss = attention_kl_loss(student_attns, teacher_attns, mapping_attn, temperature=wandb.config.temperature)\n",
        "\n",
        "            total_loss = (\n",
        "                wandb.config.a * task_loss +\n",
        "                wandb.config.b * kl_loss +\n",
        "                wandb.config.c * attn_loss\n",
        "            )\n",
        "\n",
        "            total_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(student_model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            ema_teacher.update(student_model)\n",
        "\n",
        "            step_loss = total_loss.item()\n",
        "            step_task = task_loss.item()\n",
        "            step_kl = kl_loss.item()\n",
        "            step_attn = attn_loss.item()\n",
        "\n",
        "            epoch_task_loss += step_task\n",
        "            epoch_kl_loss += step_kl\n",
        "            epoch_attn_loss += step_attn\n",
        "            epoch_total_loss += step_loss\n",
        "\n",
        "            pb.set_postfix(loss=step_loss)\n",
        "\n",
        "            wandb.log({\n",
        "                \"step\": step + 1,\n",
        "                \"train_loss\": step_loss,\n",
        "                \"task_loss\": step_task,\n",
        "                \"kl_loss\": step_kl,\n",
        "                \"attn_loss\": step_attn\n",
        "            })\n",
        "\n",
        "            step += 1\n",
        "\n",
        "        student_model.eval()\n",
        "        ema_teacher.shadow.eval()\n",
        "\n",
        "        student_correct = 0\n",
        "        ema_correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "\n",
        "                student_outputs = student_model(input_ids, attention_mask=attention_mask)\n",
        "                student_preds = torch.argmax(student_outputs.logits, dim=1)\n",
        "                student_correct += (student_preds == labels).sum().item()\n",
        "\n",
        "                ema_outputs = ema_teacher.shadow(input_ids, attention_mask=attention_mask)\n",
        "                ema_preds = torch.argmax(ema_outputs.logits, dim=1)\n",
        "                ema_correct += (ema_preds == labels).sum().item()\n",
        "\n",
        "                total += labels.size(0)\n",
        "\n",
        "        student_val_acc = student_correct / total\n",
        "        ema_val_acc = ema_correct / total\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"student_val_acc\": student_val_acc,\n",
        "            \"ema_val_acc\": ema_val_acc,\n",
        "            \"avg_train_loss\": epoch_total_loss / len(train_loader),\n",
        "            \"avg_task_loss\": epoch_task_loss / len(train_loader),\n",
        "            \"avg_kl_loss\": epoch_kl_loss / len(train_loader),\n",
        "            \"avg_attn_loss\": epoch_attn_loss / len(train_loader),\n",
        "        })\n",
        "\n",
        "        if ema_val_acc > best_val_acc:\n",
        "            best_val_acc = ema_val_acc\n",
        "            torch.save(ema_teacher.shadow.state_dict(), \"best_ema_teacher.pt\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | Student Acc: {student_val_acc*100:.2f}% | EMA Acc: {ema_val_acc*100:.2f}%\")\n",
        "        student_model.train()\n",
        "\n",
        "    return student_model, ema_teacher.shadow"
      ],
      "metadata": {
        "id": "JzQfNBg3y7ax"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_distil_model(distil_model, tokenizer, device)    # Тут у меня в wandb всё в одном ране смешалось, я вначале не обучал учителя и не отключал дропауты, потом обучил и отключил. KL лосы сразу упали, там можно посмотреть."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "eFL2lDOT1Ywn",
        "outputId": "7a3743be-e628-46d8-d07a-904ab7215492"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 459/459 [09:08<00:00,  1.19s/it, loss=0.0276]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Student Acc: 79.48% | EMA Acc: 80.81%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 459/459 [09:16<00:00,  1.21s/it, loss=2.33]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Student Acc: 80.29% | EMA Acc: 81.10%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 459/459 [09:16<00:00,  1.21s/it, loss=0.128]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Student Acc: 81.04% | EMA Acc: 81.97%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 459/459 [09:16<00:00,  1.21s/it, loss=0.0703]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Student Acc: 81.86% | EMA Acc: 83.07%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 459/459 [09:16<00:00,  1.21s/it, loss=0.0622]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Student Acc: 80.87% | EMA Acc: 82.14%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 459/459 [09:15<00:00,  1.21s/it, loss=3.79]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 | Student Acc: 81.33% | EMA Acc: 81.39%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7:  13%|█▎        | 59/459 [01:11<08:07,  1.22s/it, loss=0.0429]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-a0ae60ff1436>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_distil_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistil_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-a690921ca15a>\u001b[0m in \u001b[0;36mtrain_distil_model\u001b[0;34m(student_model, tokenizer, device)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mema_teacher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mstep_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-9bd7686b4492>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mshadow_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_param\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshadow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mshadow_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshadow_param\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmodel_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}